{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주문집행 시뮬레이션\n",
    "- 크레프트 태크놀로지스 AXE 팀은 주문집행 문제를 강화학습을 통해 해결합니다.\n",
    "- 하지만 기존의 toy project와는 다르게, 실제 상황에 강화학습을 적용하는 시도는 생각보다 많은 장애물을 마주하게 됩니다.\n",
    "- 이 샘플 태스크는 그 응용력을 아주 간단한 범위에서 시험합니다.\n",
    "\n",
    "**주의 : '완벽하게' 푸는 것이 이 태스크의 목적은 아닙니다. 무엇보다, 자신의 생각을 코드로 구현하려는 시도가 중요합니다.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주문집행 시뮬레이션 모듈 import\n",
    "from env import OrderSimulation1, OrderSimulation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주문 집행 시뮬레이션 인스턴스 - 매수만 고려.\n",
    "env = OrderSimulation1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수 스펙 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "호가창 길이: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "274000     35\n",
       "275000    181\n",
       "276000     61\n",
       "277000     16\n",
       "278000     85\n",
       "279000    121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset -> 초기 호가정보 반환[5개의 time frame][매수 3호가 ~ 매도 3호가]\n",
    "order_books = env.reset()\n",
    "print(\"호가창 길이: {}\".format(len(order_books)))\n",
    "order_books[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_step': 13, 'mission_buy': 355}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mission_info -> 현재 episode 의 목표 구매 수량과 maximum time step\n",
    "env.mission_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274000    20\n",
       "275000    10\n",
       "276000    30\n",
       "277000     5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step -> action(매수 1호가 주문, 매수 2호가 주문, 매수 3호가 주문, 시장가 주문) 을 받아 시뮬레이션을 처리\n",
    "# 편의상 step 후 체결되지 않은 주문은 모두 취소함을 가정합니다.\n",
    "# 반환 : 새로운 호가창 시퀀스(1~4번쨰는 이전 시퀀스의 2~5번쨰 항목과 동일), 이번 기 매수체결 정보, 전체 기 매수체결 정보.\n",
    "# 모든 주문은 기존에 호가창에 있는 대기 잔량이 체결된 후에 체결됩니다.\n",
    "order_books, executed_this_step, executed_every_step = env.step([30, 10, 20, 5])\n",
    "executed_this_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[274000    141\n",
       " 275000    169\n",
       " 276000    101\n",
       " 277000    154\n",
       " 278000     48\n",
       " 279000    194\n",
       " dtype: int64,\n",
       " 273000    189\n",
       " 274000    157\n",
       " 275000    185\n",
       " 276000     47\n",
       " 277000    115\n",
       " 278000    130\n",
       " dtype: int64,\n",
       " 272000     45\n",
       " 273000    165\n",
       " 274000     77\n",
       " 275000    200\n",
       " 276000    185\n",
       " 277000    175\n",
       " dtype: int64,\n",
       " 272000     80\n",
       " 273000     18\n",
       " 274000    160\n",
       " 275000    190\n",
       " 276000    139\n",
       " 277000    103\n",
       " dtype: int64,\n",
       " 271000     14\n",
       " 272000     32\n",
       " 273000    151\n",
       " 274000    164\n",
       " 275000     50\n",
       " 276000     22\n",
       " dtype: int64,\n",
       " 271000    110\n",
       " 272000     34\n",
       " 273000     30\n",
       " 274000     17\n",
       " 275000    137\n",
       " 276000    178\n",
       " dtype: int64,\n",
       " 272000     82\n",
       " 273000    101\n",
       " 274000     38\n",
       " 275000    187\n",
       " 276000    106\n",
       " 277000    188\n",
       " dtype: int64,\n",
       " 271000     10\n",
       " 272000     29\n",
       " 273000    182\n",
       " 274000    187\n",
       " 275000     40\n",
       " 276000     21\n",
       " dtype: int64,\n",
       " 270000    126\n",
       " 271000     60\n",
       " 272000    199\n",
       " 273000    138\n",
       " 274000     14\n",
       " 275000    132\n",
       " dtype: int64,\n",
       " 269000     70\n",
       " 270000     19\n",
       " 271000    188\n",
       " 272000    104\n",
       " 273000     62\n",
       " 274000     80\n",
       " dtype: int64,\n",
       " 268000    159\n",
       " 269000    149\n",
       " 270000    161\n",
       " 271000    187\n",
       " 272000    123\n",
       " 273000     86\n",
       " dtype: int64,\n",
       " 265000    164\n",
       " 266000     19\n",
       " 267000    118\n",
       " 268000     69\n",
       " 269000     17\n",
       " 270000    119\n",
       " dtype: int64]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_left_observation() 은 일종의 치트 메소드 입니다.\n",
    "# 현재부터 episode의 끝까지 발생할 다음 호가창 상태를 미리 알려줍니다.\n",
    "# (이 환경은 상태전이와 action의 독립을 가정하고 있습니다)\n",
    "env.get_left_observation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task - 리워드가 없는데요\n",
    "강화학습의 핵심은 reward입니다. <br/>\n",
    "하지만 보시다 시피 주문집행에서 reset과 step은 명시적인 reward를 반환하지 않습니다.(사소하지만 done 역시 마찬가지입니다.) <br/>\n",
    "기존 OrderSimulation class를 이용, 아래 형식에 맞추어 Gym 환경을 다시 정의해보십시오.<br/>\n",
    "\n",
    "\n",
    "*HINT* : 이 simulation은 학습을 위한 gym입니다. <br/>\n",
    "action을 결정하는 데는 사용해서는 안 되지만, reward를 계산하는 데 미래 데이터를 쓰는데 주저하지 마십시오!\n",
    "\n",
    "*WARNING* : 정답은 없습니다. 그럴 듯한 학습 목표를 세우는 데 초점을 맞추시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrderGym:\n",
    "    \n",
    "    # TODO\n",
    "    # return: state, reward, done, info\n",
    "    def reset(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # TODO\n",
    "    # return: state, reward, done, info\n",
    "    def step(self, action):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2 - 투자왕 태희\n",
    "만약 미래의 상태 전이를 알 수 없는 경우(예컨데 온라인 학습 상황)라면 어떻게 학습을 진행할 수 있을까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'OrderSimulation2' object has no attribute 'get_left_observation'\n"
     ]
    }
   ],
   "source": [
    "# 이 환경은 미래 상태를 전혀 알 수 없습니다. 실제 동작 역시 step 시 마다 conditionally - stochastic하게 상태가 전이되는 형태입니다.\n",
    "env = OrderSimulation2()\n",
    "order_books= env.reset()\n",
    "try:\n",
    "    env.get_left_observation()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 투자왕 태희\n",
    "정말 막막한 상황입니다. 당장 학습을 해야 하는데, 시뮬레이션이 끝나기 전까지는 지금의 행동(주문)이 얼마나 효과적인 것이었는지 판단할 기준이 없습니다.<br/>\n",
    "하지만 불행중 다행히도 자신이 투자의 왕이라고 주장하는 '태희'라는 친구가 옆에서 코칭을 해 주겠다고 합니다! <br/>\n",
    "다시 강화학습을 시도할 수 있을까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 5, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taehee_action -> 자신만의 투자 방식을 가지고 있는 '태희'의 action을 제시합니다.\n",
    "# '태희'는 믿을만한 투자의 고수입니다.\n",
    "taehee_action = env.taehee_action()\n",
    "taehee_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward 정의해내기\n",
    "'태희'가 우리의 투자에 대한 reward를 제시해주지는 않습니다. 다만 우리가 투자하는 순간에 옆에서 '나라면 이렇게 하겠어' 하고 훈수를 둘 뿐이죠, <br/>\n",
    " '태희'의 훈수를 기반으로 Reward를 명시적으로 알 지 못하는 환경의 학습을 설계하십시오(세부 알고리즘은 어떤 것이어도 상관 없습니다)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
